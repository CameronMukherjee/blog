[{"content":"","date":"3 December 2023","permalink":"/posts/","section":"Blog Posts","summary":"","title":"Blog Posts"},{"content":" Preface # The technological benefits of Monoliths is a widely covered topic in the technology community, however, the benefits for an organisation/business are less discussed. This post aims to bridge that gap.\nWhat is a Monolithic Application? # A monolith is a single-tiered software application where different components are combined into a single program from a single platform. This usually leads to deeply coupled code making it harder for individual teams to work in their own independent software development life cycles (SDLCs).\nWhat is Mini-Monolith / Microservice Applications? # Mini-monolith or microservice applications are composed of small, independent modules that work together to form a complete platform. Each module can be developed, deployed, and maintained independently, with its own dedicated team and development pipeline. This independence allows for more flexibility and scalability in the software development process.\nWhy are Monoliths important? # Monoliths offer simplicity and straightforward development and deployment processes, making them initially appealing for businesses. They allow for great velocity when working on small scale projects as well as easy deployments strategies - both facilitating the ability to go to market faster.\nWhy move to Mini-Monolith / Microservices? # Deployment Blockers # Microservices reduce deployment blockers as each service is deployed independently. This means updates or fixes in one area don\u0026rsquo;t hold up the entire application\u0026rsquo;s deployment process. Many companies completely automate this process, allowing for a much faster, leaner release cycles in comparison to batch feature releases.\nFeature Branch Releases # Adopting feature branching in a microservices architecture allows for each feature to be developed and released sequentially, rather than in batched updates. This approach offers several advantages. Firstly, it enables faster rollbacks in case of issues, as only the specific feature branch is affected. Secondly, it facilitates more focused testing and quality assurance, since each feature is isolated. This method also accelerates the feedback loop from users, as features are released incrementally. Moreover, it reduces the risk of conflicts between different development efforts, leading to a smoother and more efficient release process. Feature branching thus enhances the overall agility and responsiveness of the development cycle.\nTeam Growth # Microservices support team growth by allowing new team members to focus on specific areas without needing to understand the entire codebase. This specialization accelerates onboarding and productivity.\nTeam Communication and Transparency # Implementing a platform like Slack in a microservices environment enhances team communication and transparency. A dedicated Slack channel for each microservice team creates a centralized communication hub, facilitating real-time updates, cross-departmental collaboration, and a transparent decision-making process. This setup is particularly beneficial for distributed teams, promoting an inclusive culture and efficient problem-solving. By leveraging Slack, teams can maintain a continuous flow of information, ensuring everyone is aligned and informed, which is crucial in a fast-paced development environment.\nDomain Experts # Teams can develop deep expertise in their specific service area, leading to higher quality outputs. Collaboration is enhanced as teams share their specialized knowledge across the organization.\nFaster time to market # Teams can develop, test, and deploy their services independently, leading to faster overall development cycles. This independence speeds up innovation and response to market changes. This is especially important when you consider the automation provided by modern CI/CD pipelines with the ability for releases to not be blocked, as could be the case in a Monolithic deployment.\nTeam Autonomy # Each team has control over their microservice, from development to deployment, fostering a sense of ownership and accountability, which often leads to better quality and innovation.\nFaster Releases # With smaller, independent units of code, releases can happen more frequently and with less risk, allowing the business to deliver new features and fixes to customers faster. Tests can also be focused to the specific domain of the application, allowing unit tests to be written, maintained and tested by the specific domain experts.\nHappier Customers # The combination of faster releases, better quality, and quick issue resolution often leads to improved customer satisfaction, as customers benefit from a more responsive and continuously improving product.\nTLDR - Moving from Monolith to Microservices # The easiest and most tangible way to quickly migrate a monolithic application to microservice / mini-monolith would be a divide and conquer strategy. This is where we find specific domains that our application fulfills and partition / break them out one by one.\nIn this example we want to create a new microservice for the invoice system. We can do this quickly by duplicating our original application code into a new repository and then deploying this as a new instance. At this point we have two identical instances deployed. In this example When requests are made for /invoice we simply route the request to the invoice service.\nA new team would then take ownership of the invoice code (as it has been split out into its own repository), allowing them to trim down the application code to ensure its domain specific (only related to invoicing).\nIn a real production environment you would manage the switch for /invoice routing to the invoice service via a feature flag. This ensures there is 0 downtime for the system. This also makes it possible to fall back to the original system \u0026lsquo;on the fly\u0026rsquo; if something was to go wrong during the refactor.\nOnce the refactor is complete and thoroughly tested, the original code found in the main application can be removed.\nWrapping Up # Microservices and Mini-Monoliths offer significant benefits when implemented well, but they\u0026rsquo;re not a universal solution for every business. I hope this overview was helpful in understanding their potential advantages for your organization.\n","date":"3 December 2023","permalink":"/posts/breaking-monoliths/","section":"Blog Posts","summary":"Preface # The technological benefits of Monoliths is a widely covered topic in the technology community, however, the benefits for an organisation/business are less discussed.","title":"Breaking Monoliths for Business Velocity"},{"content":"","date":"3 December 2023","permalink":"/tags/business/","section":"Tags","summary":"","title":"business"},{"content":"","date":"3 December 2023","permalink":"/tags/distributed/","section":"Tags","summary":"","title":"distributed"},{"content":"","date":"3 December 2023","permalink":"/tags/systems-design/","section":"Tags","summary":"","title":"systems design"},{"content":"","date":"3 December 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"3 December 2023","permalink":"/","section":"ufu.dev","summary":"","title":"ufu.dev"},{"content":"","date":"16 August 2023","permalink":"/tags/high-performance/","section":"Tags","summary":"","title":"high performance"},{"content":" Introduction # In today\u0026rsquo;s dynamic digital landscape, efficient request routing is paramount for ensuring optimal performance and user experience. As businesses grow and diversify their online services, the need for a more refined, percentage-based request routing system becomes evident. Such a system can be instrumental in balancing payment requests, especially when dealing with multiple payment providers that have varying cost structures. But how can we design a system that routes thousands of requests per second based on predefined percentages without compromising on performance? Moreover, how can we ensure that this system is scalable and distributed to meet the demands of a rapidly evolving digital ecosystem? In this blog post, we delve deep into two potential solutions to this intriguing challenge: the Atomic Redis Counter and the Ring Buffer with Service Discovery. Join us as we dissect each approach, weighing their pros and cons, and provide insights into which might be the best fit for your specific needs.\nProblem Statement # We want the ability to route requests to different gateways based on a percentage. This system does not need to be 100% accurate when routing requests.\nHere is an example configuration:\nRoute Percentage Twitter 50% Google 20% Facebook 30% In a real world situation this would most likely be used for percentage routing and balancing of payment requests, certain payment providers charge more per request once a specific threshold is met. A percentage based load balancer could prevent these thresholds from being met.\nTo help conceptualise the requirements we can say we have 3000 requests per second, all of which need to be routed based on this percentage.\nHow can we effectively implement a system to ensure all requests are routed based on their percentage without impacting performance while also ensuring the system is highly distributed and scalable?\nPossible Solution - Atomic Redis Counter: # A potential answer to this question could be an atomic counter using Redis.\nThe counter would start at 1 and go to 100, as requests come in each service would lock the key / value pair in Redis, read the value and update the counter.\nThe value would then determine which gateway to send the request to, for example:\n// Increment the counter atomically long counter = jedis.incr(\u0026#34;request_counter\u0026#34;); // If counter exceeds 100, reset it to 1 if (counter \u0026gt; 100) { jedis.set(\u0026#34;request_counter\u0026#34;, \u0026#34;1\u0026#34;); counter = 1; } // Determine the gateway based on the counter value if (counter \u0026lt;= 50) { return \u0026#34;Twitter\u0026#34;; } else if (counter \u0026lt;= 70) { // 50 + 20 = 70 return \u0026#34;Google\u0026#34;; } else { return \u0026#34;Facebook\u0026#34;; } While this does work, it does come with some concerns, especially seeming as our requirements requiring 3000 requests per second. Below I have listed a handful of issues with this design.\nConcerns with this approach: # Scalability: Single Point of Bottleneck: Since all requests need to interact with the Redis instance to get the counter value, it can become a bottleneck when there\u0026rsquo;s a high volume of requests. Even though Redis is fast, the sheer number of operations (increment and possibly reset) can strain the system. Network Latency: If the application instances are not co-located with the Redis instance, network latency can introduce delays in request routing. Possibility to block other instances / servers: Lock Contention: If multiple instances try to reset the counter simultaneously after it exceeds 100, there can be contention. While Redis operations like INCR are atomic, the check-and-set pattern (checking if the counter is over 100 and then resetting it) is not inherently atomic and can lead to race conditions. Blocking Delays: If one instance takes longer to interact with Redis (due to network issues, for instance), it might delay other instances from accessing the counter, leading to uneven distribution or potential timeouts. Possible Cascading Failures: Cascade Failures: If the Redis instance becomes unresponsive or slow (due to high CPU usage, memory issues, etc.), it can cause the application instances to become slow or unresponsive as they wait for Redis. This can lead to a cascading failure where one component\u0026rsquo;s failure leads to system-wide degradation or outage. Single Point of Failure: If the Redis instance goes down, the entire request routing mechanism breaks. Even with Redis replication, there\u0026rsquo;s a period of time during failover where the system might not function as expected. Thread Blocking: Since Redis is single-threaded, long-running commands or a large number of simultaneous commands can block the server, causing delays. If our application instance is also single-threaded, it can get blocked waiting for Redis, leading to potential deadlocks or severe performance degradation. Possible Solution - Ring Buffer with Service Discovery: # The code shown here in snippets can be found in this GitHub Gist.\nA more scalable solution to this issue (with additional overhead) would be implementing service discovery for each node and maintain a local copy of a Ring Buffer.\nThe ring buffer would be 100 elements long, representing the percentage distribution for routing.\nWe start with an array of 100 values # 0 to 99 - this will be used to represent the percentage routing The array is attached to itself in the following fashion, forming a ring # This is commonly known as a ring buffer, circular buffer or cyclic buffer: We can now plot our percentage based routing into the ring in the following fashion # This is an example of what the underlying array would look like:\n# 50 Twitters, 30 Facebooks, 20 Googles [https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.twitter.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.facebook.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com, https://www.google.com] Plot our server pointers onto the ring: # We can now create our pointers for each server / instance, and balance them across the ring buffer. We would utilise a distribution coordination tool like Apache ZooKeeper for this part. Apache ZooKeeper will give us the ability to find out how many instances of this server is online, allowing us to distribute tasks across the instances. Figuring out the pointer location is very easy and looks something like this:\nprivate void updateRingPointer() { int partitionSize = 100 / getInstancesAvailable(); this.pointer = getInstanceNumber() * partitionSize; log.info(\u0026#34;Instance pointer has been changed to: [{}]\u0026#34;, this.pointer); } Each service would be responsible for resolving their own pointer.\nHow do I use this in my implementation? # Important Note: When a request hits our service we first make sure the requests are load balanced via Round Robin, any non-sequential / cyclic load balancing strategy will result with this not working, for example hash based routing, or sticky routing.\nFirstly, our server instance registers with Apache ZooKeeper and finds out where there pointer is via the following code: public int getInstancesAvailable() throws InterruptedException, KeeperException { List\u0026lt;String\u0026gt; activeInstances = zooKeeper.getChildren(serverInstancePath, this); return activeInstances.size(); } public int getInstanceNumber() throws InterruptedException, KeeperException { List\u0026lt;String\u0026gt; activeInstances = zooKeeper.getChildren(serverInstancePath, this); return activeInstances.indexOf(instanceName); } private void updateRingPointer() { int partitionSize = 100 / getInstancesAvailable(); this.pointer = getInstanceNumber() * partitionSize; log.info(\u0026#34;Instance pointer has been changed to: [{}]\u0026#34;, this.pointer); } When a new request lands on our service we can convert our configuration to the ring buffer (code omitted as it\u0026rsquo;s highly dependent on your domain objects) - please make sure it builds in the same order for all instances (have some sorting involved). We fetch the pointer (which has been previously updated by our updateRingPointer() function): public int getAndIncrementPointer() { int currentPointer = this.pointer; if (currentPointer + 1 \u0026gt;= 100) { this.pointer = 0; } else { this.pointer++; } return currentPointer; } We can then index into the array at that value and route the request based on the value. If an error occurs you can decrement the pointer by 1 (if value is at 0 then return 100) to ensure you maintain a consistent balance between all nodes. Here is what this looks like across 4 different services: The first request lands on Instance 0 and moves the pointer forward by 1. The second request lands on Instance 1 and moves the pointer forward by 1. The third request lands on Instance 2 and moves the pointer forward by 1. The fourth request lands on Instance 3 and moves the pointer forward by 1. The fifth request lands on Instance 0 and moves the pointer forward by 1. When any pointer reaches 100 they are reset to 0.\nWhat about scaling up and down? # Instances will receive an event from Apache ZooKeeper stating that children have changed, allowing you to re-run the updateRingPointer() function provided above, providing the new pointer location.\nSo, what if 4 instances are now available (we\u0026rsquo;ve scaled up)? The ring buffer pointers would be updated to the following, as you can see, all pointers have been moved evenly apart to ensure a correct balance when receiving the next set of requests.\nHere are some examples of what the pointers would look like based on how many instances are available:\n4 Instances:\nInstance Number Pointer 0 0 1 25 2 50 3 75 3 Instances:\nInstance Number Pointer 0 0 1 33 2 66 Keep in mind, everytime an instance is added or removed these pointers are reset to a default position to ensure equal balancing. For example, this means if Instance 0 has moved forward 10 paces but a new instance comes online, the pointer is reset to 0 - meaning the next 10 requests will be duplicated (based on current implementation - something I would like to fix moving forward).\nAdvantages of this approach # No usage of Redis to maintain a counter, removing the possibility of thread deprivation or single point of failure. Low memory footprint if we use String values in the ring buffer (in Java, the String pool will be used for storing each gateway config). Faster lookup as array has already been build / cached on the instance (O(1) look up). Highly scalable as new nodes can join and leave the cluster without introducing any locks on shared state. Concerns with this approach # If nodes are added and removed from the cluster frequently, this will not work effectively, re-balancing will cause all pointers to be rebalanced and thus reset to a default equally partitioned state. Additional code complexity and new service introduction (Apache ZooKeeper). Overhead from building the Ring Buffer (can be cached within the instance to lower the overhead - this comes with its own downfalls though). Future Improvement # Instead of maintaining one pointer per service, we can maintain multiple (acting as virtual pointers), which allows for a much more even distribution. Ability to add and remove nodes without moving current in use pointers - allowing for re-balancing to not affect the currently processed distribution. Conclusion # Software architecture is a vast and intricate domain, where one-size-fits-all solutions are a rarity. The challenge of routing requests based on percentages, particularly at high volumes, epitomizes this complexity. In our exploration, we delved into two promising solutions: the Atomic Redis Counter and the Ring Buffer with Service Discovery.\nThe Atomic Redis Counter stands out for its straightforwardness. It\u0026rsquo;s a solution that can be quickly implemented and understood. However, its simplicity can be a double-edged sword when scalability comes into play. For systems that don\u0026rsquo;t anticipate a surge in traffic or those in their nascent stages, this might be an ideal choice.\nOn the other hand, the Ring Buffer with Service Discovery is a testament to the power of modern distributed systems. While it demands a deeper understanding and more intricate setup, it shines in scenarios where scalability and distributed processing are paramount. Systems that are poised for growth or those already experiencing high traffic volumes might find this approach more in line with their needs.\nYet, it\u0026rsquo;s crucial to remember that the world of software is teeming with possibilities. The strategies discussed here are but a drop in the ocean of potential solutions. The right approach hinges heavily on the unique challenges and requirements of your system. It\u0026rsquo;s not about finding the \u0026ldquo;best\u0026rdquo; solution in a general sense, but rather the most fitting one for your specific context.\nMoreover, the technological landscape is in a state of perpetual flux. Today\u0026rsquo;s cutting-edge solution might become tomorrow\u0026rsquo;s legacy system. As architects and developers, it\u0026rsquo;s our responsibility to stay abreast of emerging tools and techniques. Regular evaluations, performance monitoring, and a willingness to pivot when necessary are essential.\nIn wrapping up, percentage-based request routing, though a nuanced problem, offers a myriad of solutions. The essence lies in comprehending your system\u0026rsquo;s intricacies, forecasting potential hurdles, and opting for a solution that strikes the right chord between simplicity, scalability, and efficacy. In the realm of software, the optimal solution is invariably tailored to your unique challenges and aspirations.\nGitHub Gist related to this project.\n","date":"16 August 2023","permalink":"/posts/percentage-based-routing-in-distributed-systems/","section":"Blog Posts","summary":"Introduction # In today\u0026rsquo;s dynamic digital landscape, efficient request routing is paramount for ensuring optimal performance and user experience.","title":"Percentage Based Routing in High Performance Distributed Systems"},{"content":" Introduction # It is important to note that the content of this blog post primarily pertains to the UK, and more specifically, the London job market. While some information may also apply to other geographical regions, my expertise and familiarity are rooted in the context of the UK.\nRecently, I\u0026rsquo;ve observed a trend among my colleagues and acquaintances, primarily composed of junior to mid-tier engineers, who are opting for changes in their jobs, roles, and responsibilities. This is a common occurrence given their considerable growth potential.\nFrequently, I find myself assisting individuals in understanding the intricacies of the interview process, suggesting potential platforms for job search, preparing for interviews, and providing insight into the skills needed in various sectors, such as front-end or back-end development.\nThe goal of this post is to shed light on the current career landscape and arm you with the essential information you need before you embark on the job application process.\nWhere to look for positions? # If you are actively seeking a position, I recommend setting up accounts on the following websites\ncord.co # Cord is a highly recommended platform for developers actively seeking new positions. The platform offers an intuitive filter that aids in locating jobs based on your preferred job title, experience, sector, and compensation. The quality of companies featured on Cord is generally high, and the website offers a smooth user experience.\nindeed.com # Indeed is another excellent resource for developer positions, particularly when using the filter function and the \u0026lsquo;Quick Apply\u0026rsquo; feature. A useful tip when using Indeed is to input your sector, such as \u0026lsquo;Developer\u0026rsquo;, and add a filter for your preferred language. For instance, +java developer or +java engineer will yield job listings that explicitly mention java, thus facilitating a more efficient job search and application process.\nmonster.com # Although I have comparatively less experience with Monster, it is still a viable platform for job search. You might find less competitive positions here. Monster\u0026rsquo;s user interface is easy to navigate, and the platform has a \u0026lsquo;Quick Apply\u0026rsquo; feature, making it ideal for mass applications within your specified criteria.\nStreamlined Strategies for Rapid Job Search # Many job-seekers operate under the belief that they should scrutinize every job listing and ensure their skills perfectly align with the job specifications before they consider applying. However, in the current dynamic job market, this approach might not be the most efficient or successful.\nThe key to securing a job promptly is to concentrate on maximizing the number of appropriate applications you send out each day. For our purposes, an \u0026ldquo;appropriate\u0026rdquo; application is one where your skills correspond with at least 60% of the job specifications.\nHere are some practical steps to utilize this strategy on various job search platforms\nOn Cord # Employ the filter function to match your skills with the job specifications and compensation expectations. Generate a list of potential job openings. Briefly review each job listing, focusing on the requirements and compensation sections. Use a professionally crafted text template to apply to each role. This template should be generic and adaptable to cater to different recruiters. On Indeed # Utilize the filter function to match your primary skills with the job specifications and compensation requirements. Generate a list of potential job opportunities. Click on any job listing that offers a fast apply button. Review the job specification briefly to ensure your skills align with the requirements. Use the \u0026lsquo;quick apply\u0026rsquo; feature, autofill all the necessary details, and ensure that your CV is automatically attached during application. Monster shares similar functionality with Indeed, so the same strategies can be applied to this platform.\nKeep in mind, it\u0026rsquo;s impractical to fully vet every company during your initial application phase, as you have limited information about the organization\u0026rsquo;s current status or even whether they will respond to your application. A more effective approach is to maximize the number of applications you submit in a condensed time frame, then assess the potential employer once you receive a response.\nApply for That Job You Don\u0026rsquo;t Think You\u0026rsquo;ll Get # It\u0026rsquo;s common to come across job listings that seem slightly out of reach, based on the qualifications and experience they demand. You might be tempted to skip these listings, assuming you won\u0026rsquo;t get the job. However, it is crucial to recognize the potential in these situations and consider applying, despite your initial reservations.\nYou may be surprised by how often you progress to the interview stage, even when you believe your chances are slim. Employers don\u0026rsquo;t only look at qualifications; they also consider a candidate\u0026rsquo;s potential, adaptability, and willingness to learn. The fact that you took the initiative to apply can reflect positively on your motivation and confidence, traits highly valued by recruiters.\nFurthermore, job titles can be deceiving. They often don\u0026rsquo;t fully encapsulate the expectations of the role. An intimidating title might mask a role that aligns well with your skills and experience. By reading through the job description and responsibilities, you can gain a better understanding of the position and decide if it\u0026rsquo;s a good fit.\nLastly, companies often urgently need to fill positions, and your application might arrive at the perfect moment. Even if you\u0026rsquo;re not an exact match for their ideal candidate, the employer may be willing to provide training or accept alternative experience to ensure the role is filled promptly.\nIn summary, don\u0026rsquo;t let self-doubt or fear of rejection prevent you from exploring potential opportunities. Apply for that job you don\u0026rsquo;t think you\u0026rsquo;ll get; the result might just surprise you.\nCrafting an Effective Resume Tips and Strategies # Crafting a comprehensive and engaging resume is a critical step in your job application process. Given the volume of resumes that recruiters process daily, it\u0026rsquo;s essential to present your information clearly and concisely.\nOptimising Your Resume for Automated Software # One effective strategy is to include a table listing tools and technologies in your resume. This is particularly useful when automated software scans your CV, as it provides a clear overview of your skills.\nHere\u0026rsquo;s an extract from my CV:\nJava (8 to 17) Spring Boot Micronaut Microservices Agile Development Test Driven Development Distributed Systems Postgres, MySQL, Oracle JUnit 5 Articulating Your Experience Clearly # Companies often use pay bands when determining compensation. Hence, it\u0026rsquo;s important to clearly articulate your experience across different technology sectors. Doing so makes it easier for the company to evaluate your expected compensation.\nConsider creating a section titled \u0026lsquo;Experience\u0026rsquo;, where you specify the cumulative experience you have garnered through formal and informal education, contracts, client projects, and full-time work. You can present this information in a table format.\nFor example:\nFront-end Development 9 years Back-end Development 8 years Cloud (AWS) 4 year Development Operations 2 year This approach was instrumental in a personal experience when I applied for a Senior position. The company, after considering my comprehensive experience and skills, encouraged me to apply for a Technical Lead role instead.\nInitial calls with recruiters # On applying for a position, the first stage typically involves screening by a recruiter or a member of the company\u0026rsquo;s HR team. Although these individuals may not have technical backgrounds, they usually possess a reasonable understanding of technology. Hence, it is essential to articulate your technical knowledge in a language that both parties can understand.\nDuring these calls, you will typically discuss high-level information about your experience, tools, technologies, notice period, and expected compensation. You might be asked about your current salary, but remember, you are not obligated to disclose this information if you do not wish to.\nIf successful in the initial call, you will usually progress to one of the following stages:\nA take-home test. A technical interview, which may include systems design. A culture fit interview. I will not delve deeper into these stages as the expectations and process can vary widely across different companies.\nWhat job title should I apply for? # The expectations for different sectors and companies can vary substantially, but the following should serve as a general guide\nJunior Software Engineer # Responsibilities # Assist in the design, development, and implementation of software systems as per the given specifications. Write clean, scalable, and efficient code using software development best practices. Perform software debugging using appropriate testing methodologies. Collaborate with other team members and stakeholders to meet organisational objectives. Participate in code reviews under senior guidance to learn about common mistakes and best practices. Document technical knowledge in the form of notes, manuals, and flowcharts. Understand and meet the deadlines set for the completion of a part of or the entire project. Skills # Knowledge of one or more popular programming languages (e.g., Python, JavaScript, Java). Basic understanding of data structures and algorithms. Familiarity with the software development life cycle (SDLC). Problem-solving and logical thinking skills. Basic understanding of version control systems (e.g., Git). Good written and verbal communication skills. Mid-Tier Software Engineer # Responsibilities # Design, code, test, and debug complex software while developing new functionality, and enhancing existing functionality, in order to satisfy customer requirements, both internal and external. Participate in full development life cycle including requirements analysis and design. Review the design and code of junior engineers and provide feedback. Collaborate with other software developers, business analysts, software architects, and IT professionals to implement solutions. Mentor junior software engineers and share knowledge to improve team skills. Skills # Proficiency in multiple programming languages. Solid understanding of data structures, algorithms, and software design principles. Experience with database management and proper database design. Experience with testing frameworks and quality assurance. Strong problem-solving skills and attention to detail. Senior Software Engineer # Responsibilities # Lead the design, development, testing, and debugging of software applications. Develop high-level system design diagrams for program design, coding, testing, debugging and documentation. Coach and mentor junior engineers and provide code reviews. Interact with stakeholders to understand their requirements and guide the team in product development. Research and introduce new technologies and ideas for software improvements. Skills # Mastery of several programming languages and ability to pick up new ones quickly. Excellent understanding of data structures, algorithms, and software design principles. Experience in leading a software development project. Strong problem-solving skills and attention to detail. Excellent communication skills to interact with stakeholders and team members. Team Lead # Responsibilities # Lead a team of software engineers in the development of software applications. Coordinate with the project manager for necessary resources and project needs. Identify, track and communicate project progress, milestones, and risks. Review and approve code and design work of team members. Provide technical guidance and mentorship to the team. Skills # Technical proficiency in software development and familiarity with various languages and frameworks. Strong leadership skills and experience in leading a team. Excellent problem-solving skills and ability to make decisions under pressure. Excellent communication skills to interact with team members and stakeholders. Understanding of project management principles. Technical Lead # Responsibilities # Guide the technical direction of the team, including the technologies used, coding standards, and development processes. Oversee technical aspects of project delivery and solution delivery. Solve complex technical issues and make decisions on the use of resources and performance trade-offs. Collaborate with stakeholders to understand their technical needs and develop solutions. Mentor team members to develop their technical skills. Skills # Extensive technical knowledge and experience in software development. Leadership skills and experience in leading technical teams. Strong problem-solving skills and the ability to make decisions on complex technical issues. Excellent communication skills to convey technical decisions and their reasoning to non-technical stakeholders. Ability to learn and adapt to new technologies quickly. Engineering Manager # Responsibilities # Manage a team of software engineers, including their training and development, guidance, and performance evaluations. Oversee the design, development, and implementation of software systems. Collaborate with other teams, stakeholders, and customers to define the product roadmap. Allocate resources effectively for projects and manage budgets. Drive the adoption of best practices in software development within the team. Skills # Strong technical background with a solid understanding of software development processes. Strong leadership skills with the ability to manage, mentor, and develop a team. Excellent communication and interpersonal skills. Strategic thinking and problem-solving skills. Experience in project management, including scheduling, budgeting, and resource allocation. What should you revise? # As a general rule of thumb it\u0026rsquo;s good to revise the following sectors, I have also attached some example questions to help you revise further:\nYour language(s) of choice # Review and practice fundamental syntax rules, operators, and data types. Build a small project from scratch without using any resources or references. Read a piece of complex code written by someone else and try to understand it. Explain to someone else or write down how memory management works in your preferred language. Question: How would you handle exceptions in your preferred language? Your framework(s) of choice # Develop a small application that utilizes the most common features of the framework. Review the documentation to understand the core principles and best practices. Try to contribute to an open-source project that uses the same framework. Question: How would you manage state in your preferred framework? Question: How would you implement unit testing in your preferred framework? Data structures # Write functions to implement basic operations (insert, delete, search) for each data structure. Compare the time and space complexities of different data structures in different scenarios. Draw diagrams to visualize how each data structure works. Question: Why would you choose a HashTable for fast lookups and what are its disadvantages? Question: When would it be more appropriate to use a Tree instead of a LinkedList? Algorithms # Implement basic sorting algorithms from scratch. Solve problems on platforms like LeetCode and HackerRank. Analyze the time and space complexities of different algorithms. Study about greedy algorithms, dynamic programming, divide and conquer strategies. Question: Can you explain the differences between breadth-first search and depth-first search? Question: How would you approach solving a problem using dynamic programming? Communication # Practice active listening: Fully understand what others are saying before responding to prevent misunderstandings and to show respect to the speaker. Give clear and concise explanations: Try explaining a piece of your recent code or a technical concept to a non-technical friend or family member. Engage in group discussions and debates: Join programming communities, forums, or local groups to share your ideas and get feedback. Write effectively: Improve your writing skills by documenting your code, writing technical blog posts, or articles. Improve your presentation skills: Practice explaining a project or concept to a group of people. Record your presentation and review it critically for improvements. Question: How would you explain a complex technical problem to a stakeholder with no technical background? Question: Describe a situation where you had to convince your team or manager to accept your idea. What was the outcome? Question: What steps would you take to deal with a misunderstanding between team members on your project? Question: Can you explain the concept of \u0026lsquo;recursion\u0026rsquo; in simple terms as if you were speaking to someone without a computer science background? Question: How would you handle a situation where you and a colleague have different views on how to solve a problem? Tools and Technologies # The necessity of updating one\u0026rsquo;s knowledge and skill-set regarding external tools and technologies pertinent to your specific technology stack cannot be overstated. The specific tools and technologies that one should focus on, of course, depend greatly on the particular type of engineering work one is involved in. However, a good initial focus might include open-source tools and technologies that are universally beneficial, irrespective of the technology stack. Examples include Docker and other containerization tools, as well as monitoring and observability tools like the ELK stack or DataDog. For those engaged in backend development, distributed computing tools such as ZooKeeper and Kafka are worthy of investigation.\nWhile it isn\u0026rsquo;t crucial to comprehend the minute technicalities of how each tool operates, understanding the capabilities of these tools and their practical applications is of paramount importance. The familiarity and ability to use these tools can, at times, be a deciding factor in securing a position within a company.\nTake, for example, a previous systems design interview I attended where I was asked to discuss distributed scheduling. Fortuitously, I had been working on a personal project that involved solving this exact issue and had developed a compact application to demonstrate my understanding of ZooKeeper\u0026rsquo;s capabilities. If I had not invested in researching this area beforehand, I could have faltered in my response, even though I had never implemented such a system at the production level. The learning from this experience underscores the importance of staying updated with relevant tools and technologies in your field.\nStrategies for Revision # One of the most effective ways to revise and assimilate new knowledge is by actively engaging with it. Here are some strategies that can help you cement your learning\nDevelop Personal Projects # Practical implementation of concepts is an excellent way to not only test your understanding but also to consolidate your learning. By developing personal projects or demonstration applications, you can experience firsthand the challenges and intricacies involved. These hands-on projects can not only deepen your comprehension but also provide tangible evidence of your abilities, which can be beneficial when job hunting.\nDocument Your Findings # Regularly documenting your findings or starting a blog about your learning journey can help structure your thoughts and reinforce your understanding. This practice enables you to reflect on what you\u0026rsquo;ve learnt, articulate it in your own words, and identify any gaps in your knowledge. Moreover, it can serve as a useful reference for future studies and might even help others who are embarking on a similar journey.\nShare Publicly # Social platforms such as Twitter, LinkedIn, or GitHub provide an avenue for you to share your learning journey with a broader audience. Publicly sharing your progress can garner feedback from a diverse range of individuals, potentially providing new perspectives or insights that you might not have considered. It also helps build a sense of accountability, which can motivate you to stay consistent and diligent in your learning journey.\nTeach Others # If possible, consider teaching someone else what you\u0026rsquo;ve learned. The process of explaining a concept to someone else requires a deep understanding of the topic and can uncover gaps in your knowledge. It\u0026rsquo;s often said that \u0026ldquo;to teach is to learn twice\u0026rdquo;, and this can be an extremely effective method to solidify your understanding. It also allows you to give back to the community and help others learn, creating a virtuous cycle of knowledge sharing.\nConclusion # The journey to securing a desirable position in the London tech job market may appear challenging, but with the right approach, tools, and mindset, it becomes significantly more manageable. Remember to utilise the wealth of resources available, from online job platforms to community networks, all while refining your skills to align with market demands. Most importantly, don\u0026rsquo;t limit yourself. Apply for jobs that might seem slightly out of reach, keep learning, and show resilience in the face of rejection. This is a dynamic field, and the opportunity to learn, grow, and succeed is immense. I hope this guide provides you with some clarity and equips you for your job search. Best of luck with your career pursuits in the exciting landscape of the London tech job market.\n","date":"16 June 2023","permalink":"/posts/308-the-permanent-redirect-to-your-next-software-job/","section":"Blog Posts","summary":"Introduction # It is important to note that the content of this blog post primarily pertains to the UK, and more specifically, the London job market.","title":"308: The Permanent Redirect to Your Next Software Job"},{"content":"","date":"16 June 2023","permalink":"/tags/guide/","section":"Tags","summary":"","title":"guide"},{"content":"","date":"16 June 2023","permalink":"/tags/market/","section":"Tags","summary":"","title":"market"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]